{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "upset-sympathy",
   "metadata": {},
   "source": [
    "# BRAINWORKS -  Data Collection Tutorial\n",
    "[Mohammad M. Ghassemi](https://ghassemi.xyz), DATA Scholar, 2021\n",
    "\n",
    "## About\n",
    "The Data Genratation Phase involved the creation of several software utlities to [Extract, Transform and Load (ETL)](https://en.wikipedia.org/wiki/Extract,_transform,_load#:~:text=In%20computing%2C%20extract%2C%20transform%2C,than%20the%20source(s)) publicaly available data assets into an read-opimized MySQL database that powers downstream BRAINWORKS analytic functions. More specifically, we developed three tools to ingest data from the: (1) the [NIH ExPORTER Asset](https://exporter.nih.gov/ExPORTER_Catalog.aspx), (2) the PubMed API via the [Entrez Programming Utilities](https://www.ncbi.nlm.nih.gov/books/NBK25501/) and (3) the [Medical Subject Headings](https://www.nlm.nih.gov/databases/download/mesh.html) assets. \n",
    "\n",
    "<br>This iPython notebook provides an interactive overview of software tools, and how they can be used to recreate the data collected for BRAINWORKS.\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "animal-logic",
   "metadata": {},
   "source": [
    "## 0. Configuration and Utility Import:\n",
    "\n",
    "To use this notebook, you will need to update the configuration in `/configuration/config.py`. More specifically, you must update: `database.base_dir`, `database.configuration_file`, `data_directory`, `NCBI_API.NCBI_API_KEY` and `NCBI_API.rate_limit`. The other fields are optional. Following configuration, will import several code utilities that are shipped with this repository; the source code for these utilities may be found in the `/utils` directory of this repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adaptive-venture",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Your CPU supports instructions that this binary was not compiled to use: AVX2\n",
      "For maximum performance, you can install NMSLIB from sources \n",
      "pip install --no-binary :all: nmslib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "currentdir = os.getcwd()\n",
    "parentdir  = os.path.dirname(currentdir)\n",
    "sys.path.insert(0, parentdir)\n",
    "\n",
    "\n",
    "from utils.cloudComputing.storage        import storage\n",
    "from utils.documentCollector.exporter    import exporter\n",
    "from utils.documentCollector.pubmed      import pubmed\n",
    "from utils.database.database             import database\n",
    "from utils.generalPurpose                import generalPurpose as gp\n",
    "from configuration.config                import config\n",
    "from pprint                              import pprint     \n",
    "from utils.documentCollector.grid        import grid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unable-italic",
   "metadata": {},
   "source": [
    "<br>\n",
    "Next, we will initialize instances of several code utilities that will be used in the remainder of this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "modified-budget",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = database()          # The database utility handles the connection to the database, and interactions with it.         \n",
    "ex = exporter()          # The exporter utility collects data from the NIH Exporter utility \n",
    "pm = pubmed()            # The pubmed utility collects data from the PubMed API\n",
    "cs = storage()           # The storage utility backs up downloaded data to an AWS S3 Bucket\n",
    "g = grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-somewhere",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 1. SQL Table Generation\n",
    "To begin, we will create a set of MySQL Tables that will be used to store the data collected from the ExPORTER and PubMed data sources. The `generateTables()` functions in the `exporter` and `pubmed` utilities will automatically create all tables required to ingest data from the public sources following configuration. We are also using the [Global Researcher Identifier Database](https://www.grid.ac/). The following lines will download and import this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "assigned-group",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      " Creating ExPORTER Tables                       \n",
      "------------------------------------------------\n",
      "....`abstracts` table already exists; skipping creation\n",
      "....`link_tables` table already exists; skipping creation\n",
      "....`patents` table already exists; skipping creation\n",
      "....`projects` table already exists; skipping creation\n",
      "------------------------------------------------\n",
      " Creating PubMed Tables                         \n",
      "------------------------------------------------\n",
      ".... `application_types` table created\n",
      ".... `citations` table created\n",
      ".... `documents` table created\n",
      ".... `grants` table created\n",
      ".... `id_map` table created\n",
      ".... `qualifiers` table created\n",
      ".... `topics` table created\n",
      ".... `affiliations` table created\n",
      ".... `publications` table created\n",
      "------------------------------------------------\n",
      "Downloading latest GRID file from https://grid.ac/downloads\n",
      ".... files will be saved to ../data/GRID/\n",
      "------------------------------------------------\n",
      ".... Skipping download: we already have this file\n",
      "------------------------------------------------\n",
      " Creating GRID Tables                           \n",
      "------------------------------------------------\n",
      ".... `labels` table created\n",
      ".... `types` table created\n",
      ".... `relationships` table created\n",
      ".... `links` table created\n",
      ".... `institutes` table created\n",
      ".... `aliases` table created\n",
      ".... `external_ids` table created\n",
      ".... `addresses` table created\n",
      ".... `acronyms` table created\n",
      "------------------------------------------------\n",
      "Importing Latest GRID Data \n",
      "------------------------------------------------\n",
      ".... importing labels\n",
      ".... importing types\n",
      ".... importing relationships\n",
      ".... importing links\n",
      ".... importing institutes\n",
      ".... importing aliases\n",
      ".... importing external_ids\n",
      ".... importing acronyms\n",
      ".... importing addresses\n"
     ]
    }
   ],
   "source": [
    "ex.generateTables()      # Generate the required tables for the Exporter Data\n",
    "pm.generateTables()      # Generate the required tables for the Pubmed Data\n",
    "g.updateGRID()           # Generate the required tabled for the GRID Data     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broken-title",
   "metadata": {},
   "source": [
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "short-barrier",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 2. ExPORTER Data Collection\n",
    "With the database initated, we can begin collecting publically avaialable data on grants, patents, and affiliated papers from the [NIH ExPORTER Catalog](https://exporter.nih.gov/ExPORTER_Catalog.aspx). The `exporter` utility contains a function `collect()` that may be used to download, decompress and convert all publically available `.csv` data files into `JSON` format. Please note that the `collect` function can be run periodically to collect any new files posted by the NIH. That is, re-running the tool will _only fetch and process any new files that you have not previously collected_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "developing-closing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      "Downloading data from https://exporter.nih.gov/ \n",
      ".... files will be saved to data/ExPORTER/\n",
      "------------------------------------------------\n",
      "downloading `projects`CSV data from https://exporter.nih.gov/ExPORTER_Catalog.aspx?index=0 ...\n",
      ".... 72 / 82 previously downloaded\n",
      ".... downloading 10 new files\n",
      "downloading `abstracts`CSV data from https://exporter.nih.gov/ExPORTER_Catalog.aspx?index=1 ...\n",
      ".... 72 / 82 previously downloaded\n",
      ".... downloading 10 new files\n",
      "downloading `patents`CSV data from https://exporter.nih.gov/ExPORTER_Catalog.aspx?index=3 ...\n",
      ".... 0 / 1 previously downloaded\n",
      ".... downloading 1 new files\n",
      "downloading `link_tables`CSV data from https://exporter.nih.gov/ExPORTER_Catalog.aspx?index=5 ...\n",
      ".... 41 / 41 previously downloaded\n",
      "------------------------------------------------\n",
      " Unzipping data                                 \n",
      "------------------------------------------------\n",
      "patents ...\n",
      ".... Starting update\n",
      "projects ...\n",
      ".... Starting update\n",
      "abstracts ...\n",
      ".... Starting update\n",
      "link_tables ...\n",
      ".... No updates required\n",
      "------------------------------------------------\n",
      " Converting to JSON                             \n",
      "------------------------------------------------\n",
      "patents ...\n",
      ".... Starting update\n",
      ".... Line import errors\n",
      "projects ...\n",
      ".... Starting update\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      "abstracts ...\n",
      ".... Starting update\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      ".... Line import errors\n",
      "link_tables ...\n",
      ".... No updates required\n"
     ]
    }
   ],
   "source": [
    "ex.collect(limit_to_tables = ['abstracts', 'projects', 'patents', 'link_tables'])    # Collect any new data on abstracts, projects, patents and link_tables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-panama",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Following download, we can import the data into the MySQL Tables using the `exporter` object's `importData()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "temporal-panic",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------\n",
      " Importing Data into SQL Database               \n",
      "------------------------------------------------\n",
      "Collecting list of files previously imported\n",
      ".... This may take a while depending on the size of your database\n",
      "Importing data into patents\n",
      ".... (previously imported data will be skipped unless replace_existing is True)\n",
      "Deleting patents - these records are replaced, not augmented...\n",
      "....Importing RePORTER_PATENTS_C_ALL\n",
      "Importing data into projects\n",
      ".... (previously imported data will be skipped unless replace_existing is True)\n",
      "....Importing RePORTER_PRJ_C_FY2021_037\n",
      "....Importing RePORTER_PRJ_C_FY2021_038\n",
      "....Importing RePORTER_PRJ_C_FY2021_039\n",
      "....Importing RePORTER_PRJ_C_FY2021_040\n",
      "....Importing RePORTER_PRJ_C_FY2021_041\n",
      "....Importing RePORTER_PRJ_C_FY2021_042\n",
      "....Importing RePORTER_PRJ_C_FY2021_043\n",
      "....Importing RePORTER_PRJ_C_FY2021_044\n",
      "....Importing RePORTER_PRJ_C_FY2021_045\n",
      "....Importing RePORTER_PRJ_C_FY2021_046\n",
      "....Importing RePORTER_PRJ_C_FY2021_047\n",
      "Importing data into abstracts\n",
      ".... (previously imported data will be skipped unless replace_existing is True)\n",
      "....Importing RePORTER_PRJABS_C_FY2021_037\n",
      "....Importing RePORTER_PRJABS_C_FY2021_038\n",
      "....Importing RePORTER_PRJABS_C_FY2021_039\n",
      "....Importing RePORTER_PRJABS_C_FY2021_040\n",
      "....Importing RePORTER_PRJABS_C_FY2021_041\n",
      "....Importing RePORTER_PRJABS_C_FY2021_042\n",
      "....Importing RePORTER_PRJABS_C_FY2021_043\n",
      "....Importing RePORTER_PRJABS_C_FY2021_044\n",
      "....Importing RePORTER_PRJABS_C_FY2021_045\n",
      "....Importing RePORTER_PRJABS_C_FY2021_046\n",
      "....Importing RePORTER_PRJABS_C_FY2021_047\n",
      "Importing data into link_tables\n",
      ".... (previously imported data will be skipped unless replace_existing is True)\n"
     ]
    }
   ],
   "source": [
    "ex.importData(limit_to_tables = ['abstracts', 'projects', 'patents', 'link_tables'],  # Specifies the set of tables we want to update, \n",
    "              batch_size      = 10000) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "representative-mauritius",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## 3. PubMed Data Collection\n",
    "The data we collected using the `exporter` utility contains information on grants (see `abstracts` and `projects` tables), patents (see `patents` table), and the publication ids that resulted from grant funding (see `link_tables` tables). \n",
    "\n",
    "The `link_tables` contain the PubMed identification numbers of papers that are linked to the the patents and grants, but do not contain information on the publications themselves. To collect this data, we will make use of the `pubmed` utility. More specifically, we will use the `downloadExporterPapersByPubmedId()` fucntion, which collects all pubmed papers that show up in the `link_tables` and which we have not already imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-criminal",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.getStoredDocumentList(data_path = config['data_directory'] + 'PubMed/')\n",
    "pm.downloadExporterPapersByPubmedId(write_location= '../data/PubMed', batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "increased-birth",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Once the papers are collected, we can import them into the database by collecting the list of the stored documents we want to ingest using `getStoredDocumentList()`, and then import those collected papers into a MySQL instance using the `processPapers()` function. For instance, let's collect a list of all documents from 2013 through 2021, and then process those papers. The list of documents are stored interally within the object in `pm.document_list`. We can inspect this list and/or pass it to the `processPapers()` function to store the papers in the database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "incorporated-county",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.documentCollector.pubmed      import pubmed\n",
    "from configuration.config                import config\n",
    "pm = pubmed()\n",
    "\n",
    "for year in range(2021,2018,-1):\n",
    "    print('Importing Data From', year)\n",
    "    year_str = str(year)\n",
    "    pm.getStoredDocumentList(data_path    = config['data_directory'] + 'PubMed/'+ year_str + '/')\n",
    "    pm.processPapers( paper_list          = pm.document_list,                # The list of all Documents from the data/PubMed/ directory\n",
    "                      log_folder          = 'ingest-pubmed-' + year_str,     # The log folder that keeps track of ingestion process.\n",
    "                      prevent_duplication = False,                           # `True`: we will skip any pmids that are already in the `publications` table.\n",
    "                      purge_logs          = False,                           # `True`: logs will be purged, logs are how we keep track of what was already procesed.\n",
    "                      db_insert           = True,                            # `True`: values are inserted into the database\n",
    "                      batch_size          = 1000,                            # Reccomended size: 10000, \n",
    "                      limit_to_tables     = ['affiliations','documents','id_map','grants','topics','qualifiers','citations','publications','triples','concepts']  # ['affiliations','documents','id_map','grants','topics','qualifiers','citations','publications','triples','concepts']\n",
    "                    )                  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9a42a8-36bf-4e01-87cb-f2bc7ca367b3",
   "metadata": {},
   "source": [
    "<br><br> If you are processing `triples` and `concepts` for a large number of papers, you may require parallel computing resources to accomplish the task in a reasonable timeframe. Please see the parallel computing module in the `/cluster` directory for instructions on how to configure and run the parallel computing cluster to extract paper information in-parallel. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a90f5a-652d-4ac7-ad2e-f6faa3019a6e",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "## 4. Download papers citing papers\n",
    "We can now download the papers that cited the papers we've already collected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350ea185-adff-494e-a34a-92be8bfa0d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.getStoredDocumentList(data_path = config['data_directory'] + 'PubMed/')\n",
    "pm.downloadCitedPapersByPubmedId(write_location= '../data/PubMed', batch_size = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alien-saturday",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "\n",
    "## Appendix A\n",
    "Below are some additional features that were developed, but are not critical to the core data collection procedure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-cosmetic",
   "metadata": {},
   "source": [
    "#### A.1 Compute Field Statistics\n",
    "You can compute field level statisticas on your JSON document store using the `getStoredDocumentFieldStats` function. The function will capture all unique JSON fields that show up across all records in the set, and the percentage of records that contain a given field."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-mumbai",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.getStoredDocumentFieldStats(data_path = config['data_directory'] + 'PubMed/', \n",
    "                               savename  = config['data_directory'] + 'PubMed/stats/pubmed-field-statistics.stats')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thrown-figure",
   "metadata": {},
   "source": [
    "#### A.2 Backup Files to S3 Bucket\n",
    "You may backup the contents of a directory to an S3 bucket using the `cs.backup` fucntion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-kruger",
   "metadata": {},
   "outputs": [],
   "source": [
    "files          = gp.getDirectoryContents(data_path = config['data_directory'] + 'ExPORTER/')\n",
    "failed_uploads = cs.backup(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a6615c8-ce9b-471a-a54e-c270643ef545",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploading 10525517 files\n"
     ]
    }
   ],
   "source": [
    "files          = gp.getDirectoryContents(data_path = config['data_directory'] + 'PubMed/')\n",
    "failed_uploads = cs.backup(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f83254cc-92c3-4ec1-9a97-6b8c67478a98",
   "metadata": {},
   "source": [
    "#### A.3 Purge Data\n",
    "Optionally, you can remove a set of documents from the database. In the example below, we are removing data from 2021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "454265e3-fdb2-4057-acc4-2e66f15ae6c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm.getStoredDocumentList(data_path = config['data_directory'] + 'PubMed/2021/')\n",
    "db.purgeDocumentsfromDatabase(pm.document_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
